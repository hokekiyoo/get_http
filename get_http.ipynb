{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自動で抽出するアレ\n",
    "- reference : [【https化に向けて】混在コンテンツのhttpを自動で抽出するやつを作った](http://www.procrasist.com/entry/get-http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "def extract_urls(root_url):\n",
    "    \"\"\"\n",
    "    トップページを指定すると、ブログ内に存在するurlをすべて抜き出してくれる\n",
    "    \"\"\"\n",
    "    is_articles = True\n",
    "    page = 1\n",
    "    urls = []\n",
    "    titles = []\n",
    "    while is_articles:\n",
    "        try:\n",
    "            html = request.urlopen(\"{}/archive?page={}\".format(root_url, page))\n",
    "        except urllib.error.HTTPError as e: \n",
    "            # HTTPレスポンスのステータスコードが404, 403, 401などの例外処理\n",
    "            print(e.reason)\n",
    "            break\n",
    "        except urllib.error.URLError as e: \n",
    "            # アクセスしようとしたurlが無効なときの例外処理\n",
    "            print(e.reason)\n",
    "            break\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        articles = soup.find_all(\"a\",class_=\"entry-title-link\")\n",
    "        for article in articles:\n",
    "            urls.append(article.get(\"href\"))\n",
    "            titles.append(article.text)\n",
    "        if len(articles) == 0:\n",
    "            is_articles = False\n",
    "        page += 1\n",
    "    return urls, titles\n",
    "\n",
    "def get_http(i, urls, titles):\n",
    "    \"\"\"\n",
    "    各記事内のhttpを保存\n",
    "    - gif, jpg, jpeg, png\n",
    "    \"\"\"\n",
    "    url = urls[i]\n",
    "    title = titles[i]\n",
    "    print(url,title)\n",
    "    try:\n",
    "        html = request.urlopen(url)\n",
    "    except urllib.error.HTTPError as e: \n",
    "        print(e.reason)\n",
    "    except urllib.error.URLError as e: \n",
    "        print(e.reason)\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    entry = soup.select(\".entry-content\")[0]\n",
    "    imgs = entry.find_all(\"img\")\n",
    "    scripts = entry.find_all(\"script\")\n",
    "    for script in scripts:\n",
    "        url = script.get(\"src\")\n",
    "        if url == None:\n",
    "            continue\n",
    "        if \"http://\" in url:\n",
    "                print(\"\\tscript tag:\",url)\n",
    "    for img in imgs:\n",
    "        url = img.get(\"src\")\n",
    "        if \"http://\" in url:\n",
    "            print(\"\\timg tag:\",url)\n",
    "\n",
    "def get_http_all(urls, titles):\n",
    "    \"\"\"\n",
    "    各記事内のhttpを保存\n",
    "    - gif, jpg, jpeg, png\n",
    "    \"\"\"\n",
    "    for i, (url,title) in enumerate(zip(urls, titles)):\n",
    "        count = 0\n",
    "        try:\n",
    "            html = request.urlopen(url)\n",
    "        except urllib.error.HTTPError as e: \n",
    "            print(e.reason)\n",
    "        except urllib.error.URLError as e: \n",
    "            print(e.reason)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        entry = soup.select(\".entry-content\")[0]\n",
    "        imgs = entry.find_all(\"img\")\n",
    "        scripts = entry.find_all(\"script\")\n",
    "        for script in scripts:\n",
    "            url = script.get(\"src\")\n",
    "            if url == None:\n",
    "                continue\n",
    "            if \"http://\" in url:\n",
    "                if count == 0:\n",
    "                    print(title,url)\n",
    "                print(\"\\tscript tag:\",url)\n",
    "                count += 1\n",
    "        for img in imgs:\n",
    "            url = img.get(\"src\")\n",
    "            if \"http://\" in url:\n",
    "                if count == 0:\n",
    "                    print(title,url)\n",
    "                print(\"\\timg tag:\",url)\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## この下にある、urlに自分のアドレスを入力して実行(必須)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.procrasist.com\"\n",
    "urls, titles = extract_urls(url)\n",
    "for i, (url,title) in enumerate(zip(urls, titles)):\n",
    "    print(i, title, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全記事のhttpを取得したい方はこちらを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_http_all(urls, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ひと記事ずつ見たい方はこちらを実行\n",
    "- 先ほどのリストに振られた番号を見ましょう\n",
    "- i に見たい番号を代入しましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "get_http(i, urls, titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
